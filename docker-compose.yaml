version: '3.8'
services:
  lm-eval:
    image: llm-evaluate:latest
    build: .

    # need version '3.8'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    # need version '2.3'
    # runtime: nvidia

    volumes:
      # TODO: replace /home/wazenmai/Warehouse/MoE/models/ to the model path
      - /mnt/nfs/wazenmai/results/not_all_experts_are_equal/:/models/
      # - ./results:/home/wazenmai/MoE_experiment/lm-evaluation-harness/results
      # - ./sqlite_cache:/home/wazenmai/MoE_experiments/lm-evaluation-harness/sqlite_cache
    
    # TODO: replace /models/pythia-14m to the model path
    # command: bash -c "lm_eval --model hf --model_args pretrained=/models/pythia-14m,dtype=bfloat16 --tasks mmlu --device cuda --batch_size 16 --output_path ./results --log_samples --use_cache ./sqlite_cache |& tee log1"
    command: bash -c "lm_eval --model hf --model_args pretrained=/models/mixtral-8x7B_layerwise_pruning_r6_c4_128_20240330-160959,dtype=bfloat16 --tasks boolq,rte,arc_challenge,arc_easy,hellaswag,winogrande,openbookqa,gsm8k,mmlu --device cuda --batch_size 16 --output_path ./results --log_samples --use_cache ./sqlite_cache/ |& tee log"
      
                